{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4366e065",
   "metadata": {},
   "source": [
    "# Theory Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c62fd590",
   "metadata": {},
   "source": [
    "1. What is Simple Linear Regression?\n",
    "    \n",
    "    - Simple Linear Regression is a statistical method used to model the relationship between one independent variable (X) and one dependent variable (Y) using a straight line. It predicts Y from X using the equation `Y = mX + c`, where `m` is the slope and `c` is the intercept."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0917a5a",
   "metadata": {},
   "source": [
    "2. What are the key assumptions of Simple Linear Regression?\n",
    "\n",
    "    - Linearity between X and Y\n",
    "\n",
    "    - Homoscedasticity (constant variance of errors)\n",
    "\n",
    "    - Independence of errors\n",
    "\n",
    "    - Normality of residuals\n",
    "\n",
    "    - No multicollinearity (only one X)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dacc6b28",
   "metadata": {},
   "source": [
    "3. What does the coefficient m represent in the equation Y = mX + c?\n",
    "\n",
    "    - The coefficient **m** is the **slope** of the line. It represents the change in Y for a one-unit increase in X. For example, if m = 2, then Y increases by 2 for each 1 unit increase in X.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61211c7c",
   "metadata": {},
   "source": [
    "4. What does the intercept c represent in the equation Y = mX + c?\n",
    "\n",
    "    - The intercept **c** is the value of Y when X = 0. It shows where the regression line crosses the Y-axis.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83798ad0",
   "metadata": {},
   "source": [
    "5. How do we calculate the slope m in Simple Linear Regression?\n",
    "\n",
    "    - Using the least squares method:  \n",
    "        **m = Σ((X - X̄)(Y - Ȳ)) / Σ((X - X̄)²)**  \n",
    "        This minimizes the squared differences between predicted and actual Y values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59d1eed",
   "metadata": {},
   "source": [
    "6. What is the purpose of the least squares method in Simple Linear Regression?\n",
    "\n",
    "    - It minimizes the sum of squared errors (residuals) between the actual and predicted Y values, helping to find the best-fitting line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cfba64b",
   "metadata": {},
   "source": [
    "7. How is the coefficient of determination (R²) interpreted in Simple Linear Regression?\n",
    "    \n",
    "    - R² shows how well the model explains the variance in Y.  \n",
    "        For example, R² = 0.85 means 85% of the variation in Y is explained by X.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61857638",
   "metadata": {},
   "source": [
    "8. What is Multiple Linear Regression?\n",
    "\n",
    "    - Multiple Linear Regression involves more than one independent variable to predict a dependent variable. The equation is:  \n",
    "        **Y = b₀ + b₁X₁ + b₂X₂ + ... + bnXn**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5ef25f",
   "metadata": {},
   "source": [
    "9. What is the main difference between Simple and Multiple Linear Regression?\n",
    "\n",
    "    - Simple Linear Regression uses **one independent variable**, while Multiple Linear Regression uses **two or more** independent variables to predict the outcome."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c6831e",
   "metadata": {},
   "source": [
    "10. What are the key assumptions of Multiple Linear Regression?\n",
    "\n",
    "    - Linear relationship\n",
    "\n",
    "    - No multicollinearity\n",
    "\n",
    "    - Homoscedasticity\n",
    "\n",
    "    - Independence of residuals\n",
    "\n",
    "    - Normality of residuals\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a18988",
   "metadata": {},
   "source": [
    "11. What is heteroscedasticity, and how does it affect the results of a Multiple Linear Regression model?\n",
    "\n",
    "    - Heteroscedasticity means the variance of residuals is not constant across values of X. It makes confidence intervals unreliable and affects prediction accuracy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd0722f6",
   "metadata": {},
   "source": [
    "12. How can you improve a Multiple Linear Regression model with high multicollinearity?\n",
    "\n",
    "    - Remove highly correlated predictors\n",
    "\n",
    "    - Use Principal Component Analysis (PCA)\n",
    "\n",
    "    - Combine variables\n",
    "\n",
    "    - Use regularization methods like Ridge or Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c54028ab",
   "metadata": {},
   "source": [
    "13. What are some common techniques for transforming categorical variables for use in regression models?\n",
    "    \n",
    "    - **One-hot encoding**: creates binary columns for each category\n",
    "\n",
    "    - **Label encoding**: assigns numeric values  \n",
    "            For example, \"Red\", \"Blue\" becomes [1, 2]\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3bbaa1a9",
   "metadata": {},
   "source": [
    "14. What is the role of interaction terms in Multiple Linear Regression?\n",
    "\n",
    "    - Interaction terms capture the combined effect of two variables on the dependent variable.  \n",
    "        For example, adding `X1 * X2` to the model shows how the effect of X1 on Y changes with X2.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60c76e6a",
   "metadata": {},
   "source": [
    "15. How can the interpretation of intercept differ between Simple and Multiple Linear Regression?\n",
    "\n",
    "    - In Simple Linear Regression, the intercept is the value of Y when X = 0.  \n",
    "        In Multiple Regression, it's the value of Y when all X variables = 0, which may not always be meaningful.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece7d6b1",
   "metadata": {},
   "source": [
    "16. What is the significance of the slope in regression analysis, and how does it affect predictions?\n",
    "\n",
    "    - The slope indicates the strength and direction of the relationship between X and Y.  \n",
    "        A positive slope means Y increases with X, and a negative slope means Y decreases with X.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f7e785",
   "metadata": {},
   "source": [
    "17. How does the intercept in a regression model provide context for the relationship between variables?\n",
    "\n",
    "    - It gives the baseline value of the outcome variable when all inputs are zero, helping to understand the starting point or reference level.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f206cb2c",
   "metadata": {},
   "source": [
    "18. What are the limitations of using R² as a sole measure of model performance?\n",
    "\n",
    "    - Doesn’t show if predictions are biased\n",
    "\n",
    "    - Increases with more variables, even if not useful\n",
    "\n",
    "    - Doesn’t detect overfitting  \n",
    "        Better to also use Adjusted R², RMSE, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebeae473",
   "metadata": {},
   "source": [
    "19. How would you interpret a large standard error for a regression coefficient?\n",
    "\n",
    "    - A large standard error means high uncertainty in the coefficient estimate, suggesting the variable may not be a strong predictor of Y."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d76fa0bb",
   "metadata": {},
   "source": [
    "20. How can heteroscedasticity be identified in residual plots, and why is it important to address it?\n",
    "\n",
    "    - In a residual plot, if the spread of residuals increases with X (funnel shape), it suggests heteroscedasticity.  \n",
    "        It’s important to fix it as it can lead to unreliable predictions and biased test results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fde7533",
   "metadata": {},
   "source": [
    "21. What does it mean if a Multiple Linear Regression model has a high R² but low adjusted R²?\n",
    "\n",
    "    - It means irrelevant variables are included. Adjusted R² penalizes for extra predictors, so a drop indicates they don’t add value.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddfceed",
   "metadata": {},
   "source": [
    "22. Why is it important to scale variables in Multiple Linear Regression?\n",
    "\n",
    "    - Scaling ensures all features contribute equally, especially when using regularization techniques like Ridge or Lasso that are sensitive to scale."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818fe009",
   "metadata": {},
   "source": [
    "23. What is polynomial regression?\n",
    "\n",
    "    - Polynomial regression fits a nonlinear relationship using a polynomial equation.  \n",
    "        Example: Y = b0 + b1X + b2X² + b3X³"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c45e576c",
   "metadata": {},
   "source": [
    "24. How does polynomial regression differ from linear regression?\n",
    "\n",
    "    - Linear regression fits a straight line, while polynomial regression fits a curved line by including higher-degree terms (X², X³, etc.).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "072d1afe",
   "metadata": {},
   "source": [
    "25. When is polynomial regression used?\n",
    "\n",
    "    - It’s used when the data shows a **nonlinear** trend, like curves or bends, which linear models can’t capture."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc0d1ff3",
   "metadata": {},
   "source": [
    "26. What is the general equation for polynomial regression?\n",
    "\n",
    "    - Y = b0 + b1X + b2X² + b3X³ + ... + bnXⁿ  \n",
    "        Where **n** is the degree of the polynomial."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ba5aad0",
   "metadata": {},
   "source": [
    "27. Can polynomial regression be applied to multiple variables?\n",
    "\n",
    "    - Yes, but it becomes complex. You can include interactions and powers of multiple features (e.g., X1², X1*X2, etc.).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c896a8db",
   "metadata": {},
   "source": [
    "28. What are the limitations of polynomial regression?\n",
    "\n",
    "    - Prone to **overfitting** with high-degree polynomials\n",
    "\n",
    "    - Harder to interpret\n",
    "\n",
    "    - Can lead to **unstable** models for extrapolation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4759d289",
   "metadata": {},
   "source": [
    "29. What methods can be used to evaluate model fit when selecting the degree of a polynomial?\n",
    "\n",
    "    - Use **cross-validation**\n",
    "\n",
    "    - Check **adjusted R²**\n",
    "\n",
    "    - Look at **RMSE (Root Mean Squared Error)**\n",
    "\n",
    "    - Use **learning curves**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7581f007",
   "metadata": {},
   "source": [
    "30. Why is visualization important in polynomial regression?\n",
    "\n",
    "    - It helps identify overfitting, underfitting, and the shape of the curve, making it easier to understand how well the model fits the data.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e209dc",
   "metadata": {},
   "source": [
    "31. How is polynomial regression implemented in Python?\n",
    "\n",
    "    - Using `PolynomialFeatures` and `LinearRegression` from `sklearn`:\n",
    "\n",
    "        ```python\n",
    "        from sklearn.preprocessing import PolynomialFeatures  \n",
    "        from sklearn.linear_model import LinearRegression  \n",
    "        poly = PolynomialFeatures(degree=2)  \n",
    "        X_poly = poly.fit_transform(X)  \n",
    "        model = LinearRegression().fit(X_poly, y)\n",
    "        ```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
